# Google-Advanced-Data-Analytics-Tik-Tok-Project
This is my completion of the TikTok guided project offered by the Google Advanced Data Analytics Certificate. Within this project, there are different components which follow the data analytics process: Exploratory Data Analysis, Data Cleaning and Transformation, creating Visualizations, Hypothesis Testing, Linear and Logistic Regression, and some Machine Learning.

The 1st coding portion of the project, **TikTok-Project-Understanding-the-data**, is where I gained my initial experience with the standard operational packages: pandas and numpy. I used them to filter and display the data (still in table form). I was able to familiarize myself with masking and groupby to separate the data as needed.

The 2nd portion of the project, **TikTok-Project-EDA-Cleaning-Visualizations**, focused majorly on visualizations. I was introduced to some commonly used visualization packages: matplotlib and seaborn and with those two packages I created a variety of graphs and charts to create deeper insight for the dataset I was working with. There was also some data cleaning steps involved to handle missing data.

The 3rd portion of the project, **TikTok-Project-Hypothesis-Testing**, applied some of the statistical concepts required to determine the statistical significance of variables and their impact. In the course associated with this portion of the project, I learned about confidence level, p-value, null-hypothesis, Type I and II errors, z-score (which leads into z-tests), and t-tests. The project itself applied a two-sample t-test to check if a variable "video_view_count" was affected by the binary categorical variable "verified_status". Because there are two independent groups and we are comparing their "video_view_count" which was not a normal distribution.

The 4th portion of the project, **Tiktok-Project-Logistic-Regression**, and its respective course was where I was introduced to the basicis of linear and logistic regression. Regarding linear regression, I learned about the 4 assumptions that would make a linear regression model appropriate for the situation: Linearity, Normality, Independence, and Homoscedasticity. I also learned the commonly used performance-related metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared, and Mean Absolute Percentage Error (MAPE). Regarding logistic regression (which is also what the project fell under), I learned about the ROC curve and AUC (Area Under Curve metric to describe performance of a logistic regression model) as well as True Positives, False Positives, True Negatives, False Negatives. The course only delved into logistic regression for the case of binary categorical variables. Data preprocessing was necessary to prepare the features for the model. I was introduced to the correlation matrix as an aide for variable selection and warnings of overfitting the model if too many were selected and underfitting if too little are selected or if the model is inappropriate. After the model was trained and predicted on the test set, model performance review tools like the confusion matrix and the Accuracy, Precision, Recall, and F1 Score metrics allowed me to understand the results of my model.

The 5th portion of the project **TikTok-Project-RandomForest-XGBoost-Machine-Learning**, broadened the scope of my knowledge of machine learning into 2 categories: supervised and unsupervised. And within the variety of machine learning models, the respective course dove into the K-means algorithm (an unsupervised white-box algorithm), followed by decision-trees, the random-forest model, and  the XGBoost model. The course described the concept of ensembles and bagging (boostraping and aggregation). This portion of the project was where our main objective: delivering a classification model based on the binary categorical variable "claim_status" was executed. The dataset was split into training, testing, and validation (the XGBoost model used a 5-fold split, whilst the random forest model used my custom split), the relevant features and dependent variables were seperated and preprocessing was conducted using pandas.get_dummies(), and the data was fed into SKLearn's Random Forest Classifier and xgboost's XGBoost Classifier and results were compiled. And both classification models scored 99% or above on Accuracy, Precision, Recall, and F1 Score on our test data.
